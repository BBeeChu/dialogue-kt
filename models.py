import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from trl import AutoModelForCausalLMWithValueHead
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

from utils import device

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

def get_checkpoint_path(model_name: str):
    return f"saved_models/{model_name}"

def get_base_model(base_model_name: str, tokenizer: AutoTokenizer, quantize: bool):
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        pad_token_id=tokenizer.pad_token_id,
        quantization_config=bnb_config if quantize else None,
        torch_dtype=torch.float32, # This seems helpful for train/test time consistency
        device_map={"": 0}
    )
    base_model.config.use_cache = False
    base_model.config.pretraining_tp = 1
    return base_model

def get_model(base_model_name: str, test: bool,
              model_name: str = None, pt_model_name: str = None,
              r: int = 32, lora_alpha: int = 16,
              quantize: bool = True, include_value_head: bool = False, use_gradient_checkpointing: bool = True):
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side="right")
    tokenizer.pad_token = tokenizer.bos_token # Have to pick some token, and eos triggers a warning
    model = get_base_model(base_model_name, tokenizer, quantize)
    if test and model_name:
        # Note we are loading adapter on quantized model and not merging
        # Recommended here - https://huggingface.co/docs/trl/main/en/dpo_trainer#downsides-to-merging-qlora-before-dpo-approach-2
        # Also prevents empty responses generated by Llama models
        model = PeftModel.from_pretrained(model, get_checkpoint_path(model_name))
    elif not test:
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)
        if pt_model_name:
            model = PeftModel.from_pretrained(model, pt_model_name, is_trainable=True, adapter_name="default")
            # Load same lora weights to separate adapter to be used as reference model
            model.load_adapter(pt_model_name, adapter_name="lora_ref")
        else:
            peft_config = LoraConfig(
                # TODO: adapt lm_head?
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                # target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"],
                r=r,
                lora_alpha=lora_alpha,
                lora_dropout=0.05,
                task_type="CAUSAL_LM",
                inference_mode=False,
            )
            model = get_peft_model(model, peft_config)
        # Wrap model to add (newly initialized) value head for PPO training
        if include_value_head:
            model = AutoModelForCausalLMWithValueHead(model).to(device)
            model.is_peft_model = True # Tells PPO trainer to disable adapters to recover reference model
    return model, tokenizer
